{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Scraping All Headlines"
      ],
      "metadata": {
        "id": "vqkTZJCb63If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target HTML Elements"
      ],
      "metadata": {
        "id": "2MlTbT1o6yKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start = https://turnbackhoax.id/page/39\n",
        "\n",
        "End = https://turnbackhoax.id/page/226\n",
        "\n",
        "\n",
        "Div = `<div class=\"mh-loop-content mh-clearfix\">`\n",
        "\n",
        "Title = `<h3 class=\"entry-title mh-loop-title\">`\n",
        "\n",
        "URL = `<a href>`\n",
        "\n",
        "`<div class=\"mh-meta mh-loop-meta\">`\n",
        "\n",
        "Date = `<span class=\"mh-meta-date updated\"> <i class=\"far fa-clock\">`\n",
        "\n",
        "Author = `<span class=\"mh-meta-author author vcard\"> <a class=\"fn\">`\n",
        "\n",
        "\n",
        "Preview = `<class=\"mh-excerpt\"> <p>`\n",
        "\n",
        "Image = `<figure class=\"mh-loop-thumb\"> <a href=>`"
      ],
      "metadata": {
        "id": "G8qBKFxef6f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Headlines Scraper"
      ],
      "metadata": {
        "id": "CYRI5v3c7Sz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Imports"
      ],
      "metadata": {
        "id": "hexJlgbgGJM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import re"
      ],
      "metadata": {
        "id": "hpM3RgrPjeqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape Headlines"
      ],
      "metadata": {
        "id": "Nrjq67kPGKZg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntKxSDJXfdv9"
      },
      "outputs": [],
      "source": [
        "def scrape_turnbackhoax_page(page_number, headers):\n",
        "    url = f\"https://turnbackhoax.id/page/{page_number}/\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        data = {\n",
        "            'title': [],\n",
        "            'url': [],\n",
        "            'preview': [],\n",
        "            'image_url': [],\n",
        "            'date': [],\n",
        "            'author': []\n",
        "        }\n",
        "\n",
        "        articles = soup.find_all('article', class_='mh-loop-item')\n",
        "\n",
        "        for article in articles:\n",
        "            # Extract title and URL\n",
        "            title_element = article.find('h3', class_='entry-title')\n",
        "            if title_element and title_element.a:\n",
        "                data['title'].append(title_element.a.text.strip())\n",
        "                data['url'].append(title_element.a['href'])\n",
        "            else:\n",
        "                data['title'].append('')\n",
        "                data['url'].append('')\n",
        "\n",
        "            # Extract preview text\n",
        "            preview_element = article.find('div', class_='mh-excerpt')\n",
        "            if preview_element and preview_element.p:\n",
        "                preview_text = preview_element.p.text.split('[…]')[0].strip()\n",
        "                data['preview'].append(preview_text)\n",
        "            else:\n",
        "                data['preview'].append('')\n",
        "\n",
        "            # Extract image URL\n",
        "            figure_element = article.find('figure', class_='mh-loop-thumb')\n",
        "            if figure_element and figure_element.a and figure_element.a.img:\n",
        "                data['image_url'].append(figure_element.a.img['src'])\n",
        "            else:\n",
        "                data['image_url'].append('')\n",
        "\n",
        "            # Extract date\n",
        "            date_element = article.find('span', class_='mh-meta-date')\n",
        "            if date_element:\n",
        "                data['date'].append(date_element.text.strip())\n",
        "            else:\n",
        "                data['date'].append('')\n",
        "\n",
        "            # Extract author\n",
        "            author_element = article.find('span', class_='mh-meta-author')\n",
        "            if author_element and author_element.a:\n",
        "                data['author'].append(author_element.a.text.strip())\n",
        "            else:\n",
        "                data['author'].append('')\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"\\nError on page {page_number}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"\\nUnexpected error on page {page_number}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_all_pages():\n",
        "    # Headers to mimic browser request\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    # Initialize empty list to store DataFrames from each page\n",
        "    all_data = []\n",
        "\n",
        "    # Define start and end page numbers\n",
        "    start_page = 39\n",
        "    end_page = 226\n",
        "\n",
        "    # Create progress bar\n",
        "    pbar = tqdm(range(start_page, end_page + 1), desc=\"Scraping pages\")\n",
        "\n",
        "    for page_num in pbar:\n",
        "        # Update progress bar description\n",
        "        pbar.set_description(f\"Scraping page {page_num}\")\n",
        "\n",
        "        # Scrape the current page\n",
        "        df = scrape_turnbackhoax_page(page_num, headers)\n",
        "\n",
        "        if df is not None and not df.empty:\n",
        "            # Add page number column\n",
        "            df['page_number'] = page_num\n",
        "            all_data.append(df)\n",
        "\n",
        "        # Add a delay between requests to be polite to the server\n",
        "        time.sleep(1)\n",
        "\n",
        "    if all_data:\n",
        "        # Combine all DataFrames\n",
        "        final_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "        # Save to CSV\n",
        "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f'turnbackhoax_data_{timestamp}.csv'\n",
        "        final_df.to_csv(filename, index=False)\n",
        "\n",
        "        print(f\"\\nScraping completed successfully!\")\n",
        "        print(f\"Total articles scraped: {len(final_df)}\")\n",
        "        print(f\"Data saved to: {filename}\")\n",
        "\n",
        "        return final_df\n",
        "    else:\n",
        "        print(\"\\nNo data was scraped successfully.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "uQ3bzuqEjiJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting the scraping process...\")\n",
        "    df = scrape_all_pages()\n",
        "    if df is not None:\n",
        "        print(\"\\nSample of scraped data:\")\n",
        "        print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwrbpmOclhsq",
        "outputId": "231e0d5d-cad1-4661-a3f5-467062445046"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the scraping process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping page 226: 100%|██████████| 188/188 [06:58<00:00,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping completed successfully!\n",
            "Total articles scraped: 3760\n",
            "Data saved to: turnbackhoax_data_20250725_065758.csv\n",
            "\n",
            "Sample of scraped data:\n",
            "                                               title  \\\n",
            "0     [SALAH] Sidang Jokowi Dituntut Ratusan Triliun   \n",
            "1  [SALAH] Hakim Kasus Harvey Moeis Terima Uang S...   \n",
            "2  [SALAH] Mahfud MD Resmi Dilantik sebagai Jaksa...   \n",
            "3  [SALAH] Bela Jokowi, Kader PDIP Caci Maki Hast...   \n",
            "4  [PENIPUAN] Akun Facebook “mr.terimakasih berba...   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://turnbackhoax.id/2025/01/07/salah-sidan...   \n",
            "1  https://turnbackhoax.id/2025/01/07/salah-hakim...   \n",
            "2  https://turnbackhoax.id/2025/01/07/salah-mahfu...   \n",
            "3  https://turnbackhoax.id/2025/01/07/salah-bela-...   \n",
            "4  https://turnbackhoax.id/2025/01/06/penipuan-ak...   \n",
            "\n",
            "                                             preview  \\\n",
            "0  Tidak ditemukan informasi atau pemberitaan kre...   \n",
            "1  Tidak ditemukan artikel atau informasi yang me...   \n",
            "2  Mahfud MD dalam akun Instagram resminya menjel...   \n",
            "3  Tidak ditemukan informasi atau pemberitaan kre...   \n",
            "4  Akun Facebook “mr.terimakasih berbagi Indonesi...   \n",
            "\n",
            "                                           image_url             date  \\\n",
            "0  https://turnbackhoax.id/wp-content/uploads/202...  January 7, 2025   \n",
            "1  https://turnbackhoax.id/wp-content/uploads/202...  January 7, 2025   \n",
            "2  https://turnbackhoax.id/wp-content/uploads/202...  January 7, 2025   \n",
            "3  https://turnbackhoax.id/wp-content/uploads/202...  January 7, 2025   \n",
            "4  https://turnbackhoax.id/wp-content/uploads/202...  January 6, 2025   \n",
            "\n",
            "           author  page_number  \n",
            "0  Tim Kalimasada           39  \n",
            "1  Tim Kalimasada           39  \n",
            "2  Tim Kalimasada           39  \n",
            "3  Tim Kalimasada           39  \n",
            "4   Adi Syafitrah           39  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Scraping Full Content"
      ],
      "metadata": {
        "id": "47S0MtR767yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Up HTML"
      ],
      "metadata": {
        "id": "vm5dfD79GZsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html_content(html_content):\n",
        "    \"\"\"\n",
        "    Clean HTML content and format it into readable paragraphs\n",
        "    \"\"\"\n",
        "    # Remove script and style elements\n",
        "    for script in html_content.find_all(['script', 'style']):\n",
        "        script.decompose()\n",
        "\n",
        "    # Get text and clean it\n",
        "    text = html_content.get_text(separator=' ')\n",
        "\n",
        "    # Remove extra whitespace and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "\n",
        "    # Create proper paragraphs\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    cleaned_paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "    return '\\n\\n'.join(cleaned_paragraphs)\n"
      ],
      "metadata": {
        "id": "90C2SyM-7Bwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape Articles"
      ],
      "metadata": {
        "id": "N65Op8XgGb2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_article(url):\n",
        "    \"\"\"\n",
        "    Scrape specific elements from a turnbackhoax article\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Get category\n",
        "        category_element = soup.find('span', class_='entry-meta-categories')\n",
        "        category = category_element.a.text if category_element and category_element.a else ''\n",
        "\n",
        "        # Get article content\n",
        "        content_div = soup.find('div', class_='entry-content mh-clearfix')\n",
        "        content = clean_html_content(content_div) if content_div else ''\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'category': category,\n",
        "            'content': content\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError scraping {url}: {e}\")\n",
        "        return {\n",
        "            'url': url,\n",
        "            'category': '',\n",
        "            'content': f'Error: {str(e)}'\n",
        "        }\n"
      ],
      "metadata": {
        "id": "h0qvhdYI9_UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_google_sheet():\n",
        "    \"\"\"\n",
        "    Process Google Sheet and scrape articles\n",
        "    \"\"\"\n",
        "    # URL of the Google Sheet (export as CSV)\n",
        "    sheet_id = \"\"\n",
        "    sheet_name = \"\"  # gid from the URL\n",
        "    csv_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={sheet_name}\"\n",
        "\n",
        "    try:\n",
        "        # Read Google Sheet\n",
        "        print(\"Reading Google Sheet...\")\n",
        "        df = pd.read_csv(csv_url)\n",
        "\n",
        "        # Filter rows where column H is True\n",
        "        df_filtered = df[df.iloc[:, 7] == True]  # Column H is index 7\n",
        "        urls = df_filtered.iloc[:, 2].tolist()  # Column C is index 2\n",
        "\n",
        "        print(f\"Found {len(urls)} URLs to process\")\n",
        "\n",
        "        # Initialize list to store results\n",
        "        results = []\n",
        "\n",
        "        # Process each URL with progress bar\n",
        "        for url in tqdm(urls, desc=\"Scraping articles\"):\n",
        "            result = scrape_article(url)\n",
        "            results.append(result)\n",
        "            time.sleep(1)  # Be polite to the server\n",
        "\n",
        "        # Create DataFrame from results\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Save to CSV with timestamp\n",
        "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f'turnbackhoax_articles_{timestamp}.csv'\n",
        "        results_df.to_csv(filename, index=False)\n",
        "\n",
        "        print(f\"\\nScraping completed successfully!\")\n",
        "        print(f\"Total articles scraped: {len(results_df)}\")\n",
        "        print(f\"Data saved to: {filename}\")\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing Google Sheet: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ojaNhbP6-EPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting the scraping process...\")\n",
        "    df = process_google_sheet()\n",
        "    if df is not None:\n",
        "        print(\"\\nSample of scraped data:\")\n",
        "        print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYUuWhBc-F_i",
        "outputId": "498b79b8-5ec4-4d13-e5cd-d768b1046cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting the scraping process...\n",
            "Reading Google Sheet...\n",
            "Found 3760 URLs to process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping articles: 100%|██████████| 3760/3760 [2:23:42<00:00,  2.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scraping completed successfully!\n",
            "Total articles scraped: 3760\n",
            "Data saved to: turnbackhoax_articles_20250725_122137.csv\n",
            "\n",
            "Sample of scraped data:\n",
            "                                                 url               category  \\\n",
            "0  https://turnbackhoax.id/2025/01/07/salah-sidan...  Fitnah / Hasut / Hoax   \n",
            "1  https://turnbackhoax.id/2025/01/07/salah-hakim...  Fitnah / Hasut / Hoax   \n",
            "2  https://turnbackhoax.id/2025/01/07/salah-mahfu...  Fitnah / Hasut / Hoax   \n",
            "3  https://turnbackhoax.id/2025/01/07/salah-bela-...  Fitnah / Hasut / Hoax   \n",
            "4  https://turnbackhoax.id/2025/01/06/penipuan-ak...  Fitnah / Hasut / Hoax   \n",
            "\n",
            "                                             content  \n",
            "0  Tidak ditemukan informasi atau pemberitaan kre...  \n",
            "1  Tidak ditemukan artikel atau informasi yang me...  \n",
            "2  Mahfud MD dalam akun Instagram resminya menjel...  \n",
            "3  Tidak ditemukan informasi atau pemberitaan kre...  \n",
            "4  Akun resmi Sergei Domogatski atau yang dikenal...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Cleansing Article Content"
      ],
      "metadata": {
        "id": "r8axQoiuEn3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "ME0oWZCaGjt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the Google Sheet (export as CSV)\n",
        "sheet_id = \"\"\n",
        "sheet_name = \"\"  # gid from the URL\n",
        "csv_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={sheet_name}\"\n",
        "\n",
        "# Read the sheet\n",
        "df = pd.read_csv(csv_url)\n",
        "\n",
        "def process_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    # Pattern for continuous \"=\" (like \"=====\" of any length)\n",
        "    text = re.sub(r'={2,}', r'\\n\\g<0>\\n', text)\n",
        "\n",
        "    # Pattern for spaced \"=\" (like \"= = =\" with any number of \"=\")\n",
        "    text = re.sub(r'(?:=\\s+){2,}=', r'\\n\\g<0>\\n', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Process column G starting from row 2\n",
        "df.iloc[1:, 6] = df.iloc[1:, 6].apply(process_text)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('processed_sheet.csv', index=False)\n",
        "\n",
        "# Save to text file\n",
        "with open('processed_column_g.txt', 'w', encoding='utf-8') as f:\n",
        "    for text in df.iloc[1:, 6]:\n",
        "        f.write(text + '\\n\\n')\n",
        "\n",
        "print(\"Sample of processed text:\")\n",
        "print(df.iloc[1, 6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JGkmvcLEsKZ",
        "outputId": "a709f0f9-9cc7-4831-8368-a805dc240a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of processed text:\n",
            "Tim Kalimasada\n"
          ]
        }
      ]
    }
  ]
}